
# VERL Experiments with Qwen Models

This repository contains a collection of experiments, reproducible scripts, performance logs, and insights from running **VERL** (Volcengine Reinforcement Learning) with **Qwen2.5-0.5B** and **Qwen2-7B-Instruct** on the **GSM8K** dataset using 8Ã—A100-80GB GPUs.

The goal is to explore PPO and GRPO training in VERL, evaluate throughput and memory usage under different rollout and parallelization configurations, and prepare meaningful contributions for the VERL community.

---

## ğŸ” Project Overview

VERL (https://github.com/volcengine/verl) is a flexible and scalable RL training framework with modular rollout and training backends. This repo demonstrates:

- How to train large language models (LLMs) using PPO and GRPO in VERL.
- Performance tuning across configurations (rollout.n, TP, KL types).
- Efficient multi-GPU training using FSDP and vLLM.
- Contribution-ready training scripts and engineering extensions.

---

## ğŸ“ Directory Structure

```
play-verl/
â”‚
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ tuning/
â”‚       â”œâ”€â”€ qwen2_7b_gsm8k_8gpu_recommended.sh  # high-throughput config
â”‚       â””â”€â”€ qwen2_7b_gsm8k_8gpu_min.sh          # memory-efficient config
â”‚
â”œâ”€â”€ wandb_logs/
â”‚   â””â”€â”€ ...                                     # screenshots or exports from WandB
â”‚
â”œâ”€â”€ plots/
â”‚   â””â”€â”€ throughput_vs_rollout.png              # comparison plots (tokens/s, memory)
â”‚
â””â”€â”€ README.md                                   # this file
```

---

## ğŸš€ Reproducible Training Scripts

### âœ… Recommended: High Throughput (rollout.n=5, TP=2)
- **Model**: Qwen2-7B-Instruct
- **Algorithm**: PPO
- **Parallelism**: TP=2
- **Rollout Batch Size**: 5
- **Offloading**: Disabled
- **Token Length**: 12000
- **Dynamic BSZ**: Enabled

ğŸ“„ Script: `examples/tuning/qwen2_7b_gsm8k_8gpu_recommended.sh`

### âœ… Minimum: Memory-Efficient (rollout.n=1, TP=1)
- **Model**: Qwen2-7B-Instruct
- **Algorithm**: PPO
- **Parallelism**: TP=1
- **Rollout Batch Size**: 1
- **Offloading**: Enabled
- **Token Length**: 8192
- **Dynamic BSZ**: Enabled

ğŸ“„ Script: `examples/tuning/qwen2_7b_gsm8k_8gpu_min.sh`

---

## ğŸ“Š Experimental Results

We tracked performance metrics using WandB and present comparisons below:

| Script | Model           | Algorithm | TP | Rollout.n | Dynamic BSZ | Token Len | Throughput (tok/s) | Memory Use | Notes                        |
|--------|------------------|-----------|----|-----------|-------------|-----------|---------------------|-------------|-----------------------------|
| #1     | Qwen2.5-0.5B     | GRPO      | 1  | 2         | âœ…           | 8000      |  X tok/s            | X GB        | GRPO early test             |
| #2     | Qwen2.5-0.5B     | PPO       | 1  | 1         | âŒ           | â€”         |  X tok/s            | X GB        | PPO baseline                |
| #3     | Qwen2-7B         | PPO       | 1  | 2         | âœ…           | 8192      |  X tok/s            | X GB        | Balanced config             |
| #4     | Qwen2-7B         | PPO       | 2  | 5         | âœ…           | 12000     |  X tok/s            | X GB        | Best throughput, preferred  |

ğŸ“Œ See `plots/throughput_vs_rollout.png` for visual comparison.

---

## ğŸ›  Contributions

To demonstrate engineering initiative, this repo includes:

- âœ… Two training scripts (`min`, `recommended`) for VERLâ€™s example gallery
- ğŸ§  Planned extension: add `log_gpu_memory()` utility to VERL
- ğŸ“¦ Ready-to-submit PR format (to `examples/tuning/`)
- ğŸ” Potential reward function extension module

---

## ğŸ§  System Setup

- **GPUs**: 8Ã— NVIDIA A100 (80GB)
- **Model Sizes**: Qwen2.5-0.5B and Qwen2-7B-Instruct
- **Inference Backend**: vLLM 0.8.3
- **Training Backend**: FSDP (no offload for recommended)
- **Dataset**: GSM8K (converted to Parquet format)

---

## ğŸ“® Contact

Prepared by **Xianzhong Ding** for VERL maintainers.

If you'd like a deeper dive into results, experiment details, or to review a PR draft for VERL, please reach out!

---

## ğŸ§© TODO

- [ ] Add log_gpu_memory utility module to VERL
- [ ] Prepare PR for `examples/tuning/`
- [ ] Extend PPO reward shaping function
- [ ] Evaluate additional rollout backends (SGLang)
